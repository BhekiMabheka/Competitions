{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMYbOKBT3TIETEzfqBF0pS7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BhekiMabheka/Data_Driven_Competions/blob/master/zindi_sasol_competetion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Description\n",
        "The objective of this challenge is to create a machine-learning model that can forecast the probability of each customer becoming inactive and refraining from making any transactions for a period of 90 days.\n",
        "\n",
        "An effective solution will enable a business to identify customers who may be on the verge of becoming inactive, allowing them to implement strategies in advance to retain these customers.\n",
        "\n",
        "Sasol is looking for (2 senior, 1 principal) data scientists with experience in communicating their discoveries and methodology to the business. Solutions will be requested from the top 15 users in this challenge, and 10 users residing in South Africa will be invited for a job interview at Sasol. When submitting your solution, please include your up-to-date CV.\n",
        "\n",
        "## Evaluation\n",
        "The prize-winning submission is based on the following weightings: 60% F1 score, 20% approach & methodology, 20% verbal presentation.\n",
        "\n",
        "The error metric for this competition is the F1 score, which ranges from 0 (total failure) to 1 (perfect score). Hence, the closer your score is to 1, the better your model.\n",
        "\n",
        "F1 Score: A performance score that combines both precision and recall. It is a harmonic mean of these two variables. Formula is given as: 2*Precision*Recall/(Precision + Recall)\n",
        "\n",
        "Precision: This is an indicator of the number of items correctly identified as positive out of total items identified as positive. Formula is given as: TP/(TP+FP)\n",
        "\n",
        "Recall / Sensitivity / True Positive Rate (TPR): This is an indicator of the number of items correctly identified as positive out of total actual positives. Formula is given as: TP/(TP+FN)\n",
        "\n",
        "Where:\n",
        "\n",
        "`TP=True Positive`\n",
        "`FP=False Positive`\n",
        "`TN=True Negative`\n",
        "`FN=False Negative`"
      ],
      "metadata": {
        "id": "IFmdlzWTy13L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MkljJR45Mgk6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report,confusion_matrix,ConfusionMatrixDisplay\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "import plotly.express as px\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import xgboost as xgb\n",
        "import warnings\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "xqAFJ9rwMvfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data   = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/zindi_competetions/sasol_competetion/data/Train.csv\")\n",
        "test_data    = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/zindi_competetions/sasol_competetion/data/Test.csv\")\n",
        "variables_df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/zindi_competetions/sasol_competetion/data/VariableDescription.csv\")"
      ],
      "metadata": {
        "id": "tm1DdkzszPHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data.sample(3)"
      ],
      "metadata": {
        "id": "lPvcrV-L1yn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.sample(3)"
      ],
      "metadata": {
        "id": "BxKPkJGw11ji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Examine classes and class imbalance\n",
        "\n",
        "Class imbalance means that there are unequal numbers of cases for the categories of the label. Class imbalance can seriously bias the training of classifier algorithms. It many cases, the imbalance leads to a higher error rate for the minority class. Most real-world classification problems have class imbalance, sometimes severe class imbalance, so it is important to test for this before training any model.\n",
        "\n",
        "Class imbalance can be sovled by various sampling techniques such as [stratified sampling](https://https://en.wikipedia.org/wiki/Stratified_sampling), [Synthetic Minority Over-sampling](https://https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.over_sampling.SMOTE.html#r001eabbe5dd7-1), [SVMSMOTE](https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.over_sampling.SVMSMOTE.html#imblearn.over_sampling.SVMSMOTE) etc. Imbalance have a big impact on the model metric evaluation; more especially if you're trying to optimize accuracy!"
      ],
      "metadata": {
        "id": "0FxLcWa931a8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "normalized_counts = train_data['Target'].value_counts(normalize=True).round(2)\n",
        "\n",
        "# Create a bar plot using Plotly Express\n",
        "fig = px.bar(x=normalized_counts.index, y=normalized_counts.values,\n",
        "             labels={'x': 'Target', 'y': 'Normalized Count'},\n",
        "             title='Distribution of the Target Label',\n",
        "             text=normalized_counts.values,\n",
        "             color=normalized_counts.index,\n",
        "             color_discrete_sequence=px.colors.qualitative.Set1)\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "-VEr3jpZ2Hwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize class separation by numeric features\n",
        "\n",
        "The primary goal of visualization for classification problems is to understand which features are useful for class separation."
      ],
      "metadata": {
        "id": "b0O-vZut4prP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def countplot(df, target_label):\n",
        "    \"\"\"\n",
        "    df: Pandas DataFrame\n",
        "    target_label: str, the column name for the target variable\n",
        "    \"\"\"\n",
        "    cols = df.columns.tolist()\n",
        "    for col in cols:\n",
        "        if df[col].dtypes == 'object':\n",
        "            fig = px.histogram(df, x=col, color=target_label, barmode='group',\n",
        "                               category_orders={target_label: sorted(df[target_label].unique())},\n",
        "                               labels={target_label: target_label}, width=800, height=500)\n",
        "            fig.update_layout(title_text=f'Count Plot of {col} grouped by {target_label}')\n",
        "            fig.show()\n",
        "\n",
        "countplot(df=train_data, target_label=\"Target\")"
      ],
      "metadata": {
        "id": "SceEXUvi4J21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def countplot(df, target_label):\n",
        "    \"\"\"\n",
        "    df: Pandas DataFrame\n",
        "    target_label: str, the column name for the target variable\n",
        "    \"\"\"\n",
        "    cols = df.columns.tolist()\n",
        "    for col in cols:\n",
        "        if df[col].dtypes == 'object':\n",
        "            fig = px.histogram(df, x=col, color=target_label, barmode='group',\n",
        "                               category_orders={target_label: sorted(df[target_label].unique())},\n",
        "                               labels={target_label: target_label}, width=800, height=500)\n",
        "            fig.update_layout(title_text=f'Count Plot of {col} grouped by {target_label}')\n",
        "            fig.show()\n",
        "\n",
        "countplot(df=train_data, target_label=\"damage_grade\")"
      ],
      "metadata": {
        "id": "S0402U9x4wQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preprocessing"
      ],
      "metadata": {
        "id": "aabW3FBJ5oLh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(data):\n",
        "\n",
        "    # Impute missing values for numeric features with the median\n",
        "    numeric_features = data.select_dtypes(include=['int64', 'float64']).columns\n",
        "    data[numeric_features] = data[numeric_features].fillna(data[numeric_features].median())\n",
        "\n",
        "    # Impute missing values for categorical features with the mode\n",
        "    categorical_features = data.select_dtypes(include=['object']).columns\n",
        "    data[categorical_features] = data[categorical_features].fillna(data[categorical_features].mode().iloc[0])\n",
        "\n",
        "    # Convert categorical features to numerical using one-hot encoding\n",
        "    data = pd.get_dummies(data, columns=categorical_features, drop_first=True)\n",
        "\n",
        "    return data\n",
        "\n",
        "# Invoke the function\n",
        "train_data = preprocess_data(train_data)\n",
        "test_data  = preprocess_data(test_data)"
      ],
      "metadata": {
        "id": "NVQV0TNM5nye"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}